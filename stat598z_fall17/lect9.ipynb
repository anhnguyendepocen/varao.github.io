{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lecture 9: Supervised learning\n",
    "\n",
    "### STAT598z: Intro. to computing for statistics\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Vinayak Rao\n",
    "\n",
    "#### Department of Statistics, Purdue University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=4, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised learning\n",
    "We are given training data $(X, Y) = \\{(x_1, y_1 ),\\cdots , (x_N, y_N )\\}$\n",
    "+ X: independent variables, inputs, predictors, features\n",
    "+ Y: dependent variables, outputs, response\n",
    "\n",
    "$x \\in \\mathbb{R}^P$ (usually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ regression: $y \\in \\mathbb{R}$\n",
    "+ classification: $y \\in \\{0, 1\\}$\n",
    "+ structured prediction: More complicated high-dimensional spaces with dependent components (e.g. the space of images or sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We assume $y_i = f(x_i ) + \\varepsilon_i$\n",
    "\n",
    "$\\varepsilon$ is noise (includes randomness and approximations)\n",
    "+ Independently and identically distributed (i.i.d.) according to some probability distrib. (e.g. Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given the training set $(X, Y)$, we want to estimate $f$:\n",
    "+ to study the relation between x and y\n",
    "+ to make predictions of y’s for unobserved x’s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Good predictors can be hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parametric learning\n",
    "Index functions $f$ by a finite-dimensional parameter vector\n",
    "\n",
    "E.g. linear regression\n",
    "+ Parameters are coefficients of a hyperplane\n",
    "+ Parameters have a clear interpretation\n",
    "+ Can be a bad approximation of reality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear regression\n",
    "via the `lm` function in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DataIncm <- read.table('Data/Income2.csv',header=T,sep=',')\n",
    "ggplot(DataIncm) + geom_point(aes(x=Education,y=Income))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fit <- lm(Income ~ Education, DataIncm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The first argument is a formula\n",
    "+ takes the form response ~ predictors\n",
    "+ response is a linear combination of predictors\n",
    "+ above we have just one predictor: $Education$\n",
    "+ $Income = \\beta_1 \\cdot Education + \\beta_0 + \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second argument unnecessary if variables in formula exist in current environment\n",
    "\n",
    "See documentation for other optional arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can print `fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is not all the information in `fit` (why?)\n",
    "+ Try `typeof()`, `class()`, `str()`\n",
    "+ Try plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print.default(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe fit contains the entire dataset!\n",
    "\n",
    "Can disable with `model = FALSE` option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can directly plot with ggplot :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt1 <- ggplot(DataIncm, aes(x=Education, y = Income)) +\n",
    "          geom_point(size=2, color='blue') +\n",
    "          theme(text=element_text(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt1 + geom_smooth(method='lm', se=FALSE, #Disable std. errors\n",
    "                   color='magenta', size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can regress against Seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit <- lm(Income ~ Seniority, DataIncm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can regress against both Education and Seniority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fit <- lm(Income ~ Education + Seniority, DataIncm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "+ `+` does *not* mean input is sum of Educ. and Sen.\n",
    "\n",
    "Rather: $Income = \\beta_2 \\cdot Seniority + \\beta_1 \\cdot Education + \\beta_0 + \\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the former, use `I`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit <- lm(Income ~ I(Education + Seniority), DataIncm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "+ $Income = \\beta_1 \\cdot (Seniority + Education) + \\beta_0 + \\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit <- lm(Income ~ Education + Seniority, DataIncm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we make predictions at a new set of locations?\n",
    "E.g. (15, 60) and (20, 160)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pred_locn <- data.frame(Education=c(15,20), Seniority= c(60,160))\n",
    "predict.lm(fit, pred_locn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "edu_pred <- 10:25\n",
    "sen_pred <- seq(0,200,10)\n",
    "pred <- data.frame(Education=rep(edu_pred, length(sen_pred)),\n",
    "               Seniority=rep(sen_pred, each=length(edu_pred) ))\n",
    "p_val <- predict.lm(fit, pred)\n",
    "pred_full <- cbind(pred,p_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt <- ggplot(DataIncm, aes(x=Education, y=Seniority, \n",
    "                          color=Income))+\n",
    "    geom_tile(data=pred_full, aes(x=Education, y=Seniority, \n",
    "                                  color=p_val, fill=p_val)) +\n",
    "    geom_point(size=1) + theme(text=element_text(size=10)) +\n",
    "    scale_color_continuous(low='blue', high='red') +\n",
    "    scale_fill_continuous(low='blue', high='red') +\n",
    "    geom_point(shape=1,size=1,color='black') + \n",
    "      guides(fill=FALSE) # Remove legend for 'fill'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Specifying a model for `lm`\n",
    "\n",
    "|   Symbol  | Meaning                     | Example\n",
    "|-----------|-----------------------------|----------\n",
    "|   +       | Include variable            | x + y\n",
    "|   :       | Interaction between vars    | x + y + z + x:z + y:z\n",
    "|   *       | Variables and interactions  | (x + y) * z\n",
    "|   ^       | Vars and intrcns to some order  | (x + y + z)^3\n",
    "|   -       |  Delete variable            | (x + y + z)^3 - x:y:z\n",
    "|   poly    |  Polynomial terms           | poly(x,3) + (x + y) * z\n",
    "|   I       |  New combination of vars    | I(x*y + z)\n",
    "|   1       |  Intercept                  | x - 1\n",
    "\n",
    "See documentation and http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalized linear model\n",
    "A linear model with Gaussian noise is often inappropriate.\n",
    "E.g. \n",
    "+ response is always positive\n",
    "+ count valued response \n",
    "+ {0, 1} or binary-valued as in classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better model might be:\n",
    "\n",
    "$ response = g (\\sum_{i=1}^N \\beta_i \\cdot predictor_i) + \\varepsilon$\n",
    "\n",
    "$g$ is a ‘link’ function, $\\varepsilon$ is no longer Gaussian\n",
    "\n",
    "Can fit in R with `glm()` (see documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric methods\n",
    "\n",
    "No longer limit yourself to a parametric family of functions\n",
    "\n",
    "Much more flexible\n",
    "\n",
    "Often much better prediction\n",
    "\n",
    "Complexity of $f$ can grow with size of dataset\n",
    "\n",
    "Often hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k-nearest neighbors\n",
    "\n",
    "Given training data $(X, Y)$\n",
    "\n",
    "Given a new $x^∗$ , what is the corresponding $y^∗$?\n",
    "\n",
    "Find the k-nearest neigbours of $x^∗$ . Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ **Classification**: \n",
    "Predicted $y^∗$ is the majority class-label of the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ **Regression**: Predicted $y^∗$ is the average of the $y$’s of the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3-nearest neighbors\n",
    "\n",
    "![Alt text](./figures/ht_2_14.png)\n",
    "<font size=5> (*An Introduction to Statistical Learning*, James, Witten, Hastie and Tibshirani) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Complexity of decision boundary grows with size of training set:  ‘Nonparametric’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pros:\n",
    "+ Very intuitive computational algorithm.\n",
    "+ Very easy to ‘fit’ data (you don’t, you just store it)\n",
    "+ Tends to outperform more complicated models.\n",
    "+ Easy to develop more complicated extensions E.g. locally-adaptive kNN.\n",
    "+ Exists theory for such models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cons:\n",
    "+ Cost of prediction grows linearly with training set size (can be expensive for large datasets)\n",
    "+ Tends to break down in high-dimensional spaces.\n",
    "+ Exempler-based approaches are hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "10-nearest neighbors\n",
    "\n",
    "![Alt text](./figures/ht_2_15.png)\n",
    "<font size=5> (*An Introduction to Statistical Learning*, James, Witten, Hastie and Tibshirani) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Alt text](./figures/ht_2_16.png)\n",
    "<font size=5> (*An Introduction to Statistical Learning*, James, Witten, Hastie and Tibshirani) </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ What distance function do we use? Typically Euclidean.\n",
    "+ What k do we use? Typically 3, 5, 10\n",
    "\n",
    "Usually chosen by cross-validation (more later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Large k: smooth decision boundary\n",
    "\n",
    "Small k: complex decision boundary (with local variations)\n",
    "\n",
    "+ k is a measure of model-complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How do we perform model selection?\n",
    "\n",
    "Do we prefer simple or complex models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias-variance trade-off\n",
    "\n",
    "Overly simple models\n",
    "+ cause underfitting (or bias)\n",
    "+ ignore important aspects of training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overly complex models\n",
    "+ cause overfitting (or variance)\n",
    "+ can be overly sensitive to noise in training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Complex models reduce training error, but generalize poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation\n",
    "How do we estimate generalization ability?\n",
    "Create an unseen test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cross-validation:\n",
    "+ Split your data into two sets, a training and test dataset.\n",
    "+ Fit all models on training set.\n",
    "+ Evaluate all models on test set.\n",
    "+ Pick best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Choosing k by cross-validation\n",
    "\n",
    "![Alt text](./figures/ht_2_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Often 50-50 or 70-30 training-test splits are used\n",
    "\n",
    "Too small a test set:\n",
    "+ Noisy estimates of generalization error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Too small a training set:\n",
    "+ Wasting training data\n",
    "+ Model selected using small training set may be simpler that model relevant to the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k-fold crossvalidation\n",
    "\n",
    "Split your data into k-blocks.\n",
    "\n",
    "For i = 1 to k:\n",
    "+ Fit algorithm on all except block i.\n",
    "+ Test algorithm on block i.\n",
    "Overall generalization error is the average of all errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ Can use larger training sets\n",
    "+ Can get confidence intervals on generalization error.\n",
    "\n",
    "k = N: leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### k-fold crossvalidation\n",
    "![Alt text](./figures/kfoldcv.png)\n",
    "<font size=5> (*An Introduction to Statistical Learning*, James, Witten, Hastie and Tibshirani) </font>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  },
  "livereveal": {
   "max-width": 1620,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
